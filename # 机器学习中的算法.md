# 机器学习中的算法

## 线性回归

### 1. 假设函数

$$
h_\theta(x)=x\theta=\theta^Tx
$$

matlab实现：

```matlab
h=x*theta;
```

### 2.代价函数

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}{(h_\theta(x^i)-y^i)^2}
$$

matlab实现

```matlab
J = (sum((h-y).^2))/(2*m);
```

### 3.梯度下降

$$
\theta_j=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}{((h_\theta(x^i)-y^i)x_j^i)}
$$

matlab实现：

```matlab
theta = theta - (alpha*(sum((h-y).*X))/m)'; 
```

### 4.特征缩放

$$
x=\frac{x-u}{s}
$$

u是均值，s为方差

matlab实现：

```matlab
x=（x-mean(x)）/std(x)；
```

### 5. 正规方程

$$
\theta=(x^Tx)^{-1}x^Ty
$$

matlab实现：

```matlab
theta=pinv(X'*X)*X'*y;
```

## logistic回归

### 1. 假设函数

$$
h_\theta(x)=g(\theta^Tx);
g(z)=\frac{1}{1+e^{-z}}
$$

matlab实现：

```matlab
z=x*theta;
e=exp(1);
g=1./(1.+e.^(-z));
```

### 2. 代价函数

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^ilog(h_\theta(x^i))-(1-y^i)log(1-h_\theta(x^i))]
$$

matlab实现：

```matlab
J=((-y)'*log(h)-(1-y)'*log(1-h))/m;
```

### 3.梯度下降

$$
\theta_j=\theta_j-\alpha\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x^i)-y^i)x_j^i
$$
$$
\frac{dJ(\theta)}{d\theta_j}=\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x^i)-y^i)x_j^i
$$

matlab实现：
```matlab
J=((-y)'*log(h)-(1-y)'*log(1-h))/m;
grad=((h-y)'*X)/m;
```

### 4.高级优化算法

```matlab
%输入是theta，输出是jVal和gradient，其中jVal是对照左边求损失函数的，gradient是对照左边求损失函数的偏导。
function [jVal, gradient] = costFunction(theta)
jVal = [...code to compute J(theta)...];
gradient = [...code to compute derivative of J(theta)...];
end
%Optimset函数：‘Gradobj’指用户自定义的目标函数梯度；‘MaxITer’指最大迭代次数，‘100’也就是最大迭代次数，这一项只能为整数。
options = optimset('GradObj', 'on', 'MaxIter', '100');
initialTheta = zeros(2,1);
%Fminunc函数：有三个输入，第一个输入为costfunction函数的句柄，第二个输入为设置的初始theta值，第三个输入为optimset函数的返回值。 有三个输出，optTheta为经函数计算得出的theta值，也就是损失函数最小时theta的取值，以上图为例，令损失函数取最小值的theta值都是5，下面会验证。exitFlagexitflag返回值为0或1，表示在theta点定义的损失函数是否收敛，值为1表示收敛。functionVal为costFunction函数中jVal的值。 

[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```

## 正则化

### 1.线性回归正则化

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}{(h_\theta(x^i)-y^i)^2} + \frac{\lambda}{2m}\sum_{j=1}^{n}{\theta_j^2}
$$

如果我们要使用梯度下降法令这个代价函数最小化,因为我们未对进行正则化,所以梯
度下降算法将分两种情形:
$$
\theta_0=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}{((h_\theta(x^i)-y^i)x_0^i)}
$$

$$
\theta_j=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}{((h_\theta(x^i)-y^i)x_j^i)}
$$

j=1,23...

我们同样也可以利用正规方程来求解正则化线性回归模型,方法如下所示:图中的矩阵尺寸为 (n + 1) ∗ (n + 1)。
$$
\theta=(x^Tx+\lambda\left[
\begin{matrix}
 0     & 0     & \cdots & 0      \\
0     & 1      & \cdots & 0      \\
 \vdots & \vdots & \ddots & \vdots \\
0      & 0      & \cdots & 1     \\
\end{matrix}
\right])^{-1}x^Ty
$$

### 2. 逻辑回归的正则化

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^ilog(h_\theta(x^i))-(1-y^i)log(1-h_\theta(x^i))]+\frac{\lambda}{2m}\sum_{j=1}^{n}{\theta_j^2}
$$

matlab实现：

```matlab
J=((-y)'*log(h)-(1-y)'*log(1-h))/m + sum(theta.^2)*lambda/(2*m);
```

要最小化该代价函数,通过求导,得出梯度下降算法为:

j=0:
$$
\theta_0=\theta_0-\alpha\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x^i)-y^i)x_0^i
$$

j=1,2,3...
$$
\theta_j=\theta_j-\alpha[\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x^i)-y^i)x_j^i+\frac{\lambda}{m}\theta_j]
$$


梯度表达式：

j=0时：
$$
\frac{dJ(\theta)}{d\theta_0}=\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x^i)-y^i)x_0^i
$$

j>0时：
$$
\frac{dJ(\theta)}{d\theta_j}=(\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x^i)-y^i)x_j^i)+\frac{\lambda}{m}\theta_j
$$
matlab实现：

```matlab
J=((-y)'*log(h)-(1-y)'*log(1-h))/m + sum(theta.^2)*lambda/(2*m);
grad(1,1)=X(:,1)'*(h-y)/m;%j=0
grad(2:end,1) = 1 / m * (X(:,2:end)'*(h-y)) + lambda / m * theta(2:end);%j>0
```

